"""RAG + Multi-LLM pipeline for test coverage analysis."""

from __future__ import annotations

import asyncio
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

from dotenv import load_dotenv
from langgraph.graph import END, START, StateGraph
from langgraph.runtime import Runtime
from mistralai.models import SystemMessage, UserMessage
from pydantic import BaseModel, Field
from typing_extensions import TypedDict

from agent.utils import find_scenario_files, get_mistral_client, get_vector_store

load_dotenv(Path(__file__).parent.parent.parent / ".env")

# =============================================================================
# Models
# =============================================================================


class TestCase(TypedDict):
    """Test case with coverage status."""

    id: str
    description: str
    present: bool


class ScenarioResult(TypedDict):
    """Analysis result for a scenario."""

    scenario_name: str
    scenario_path: str
    test_cases: list[TestCase]


class GeneratedTestCase(BaseModel):
    """Test case generated by LLM."""

    id: str = Field(description="Unique identifier (e.g., TC-001)")
    description: str = Field(description="Test case description")


class TestCaseList(BaseModel):
    """List of generated test cases."""

    test_cases: list[GeneratedTestCase]


class AnalyzedTestCase(BaseModel):
    """Test case with coverage status."""

    id: str
    description: str
    present: bool = Field(description="True if covered by the XML scenario")


class TestCaseAnalysis(BaseModel):
    """Coverage analysis result."""

    test_cases: list[AnalyzedTestCase]


# =============================================================================
# Context & State
# =============================================================================


@dataclass
class Context:
    """Runtime configuration for the pipeline."""

    llm1_model: str = "codestral-2501"
    llm2_model: str = "codestral-2501"
    rag_top_k: int = 1
    temperature: float = 0.0


@dataclass
class State:
    """Pipeline state."""

    req_name: str = ""
    scenario_paths: list[str] = field(default_factory=list)
    current_scenario_index: int = 0
    current_scenario_content: str = ""
    current_scenario_name: str = ""
    requirement_description: str = ""
    generated_test_cases: list[str] = field(default_factory=list)
    scenario_results: list[ScenarioResult] = field(default_factory=list)
    aggregated_test_cases: list[TestCase] = field(default_factory=list)
    errors: list[str] = field(default_factory=list)


# =============================================================================
# Nodes
# =============================================================================


async def load_scenarios(state: State, runtime: Runtime[Context]) -> dict[str, Any]:  # noqa: ARG001
    """Load the list of XML scenarios."""
    paths = await asyncio.to_thread(find_scenario_files, state.req_name)
    if not paths:
        return {"errors": [*state.errors, f"No scenarios found for {state.req_name}"]}
    return {"scenario_paths": paths}


async def retrieve_requirement(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    """Retrieve requirement description via RAG."""
    try:
        docs = await get_vector_store().asimilarity_search(
            f"Requirement {state.req_name}", k=runtime.context.rag_top_k
        )
        if not docs:
            return {
                "errors": [*state.errors, f"No document found for {state.req_name}"]
            }
        return {"requirement_description": docs[0].page_content}
    except Exception as e:
        return {"errors": [*state.errors, f"RAG error: {e}"]}


async def generate_test_cases(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    """Generate test cases for the requirement."""
    if state.errors:
        return {}

    try:
        client = get_mistral_client()
        response = await client.chat.parse_async(
            model=runtime.context.llm1_model,
            messages=[
                SystemMessage(
                    content="""You are a software testing expert.
Generate a comprehensive list of test cases for the given requirement.

For each test case:
- Unique ID (TC-XXX format)
- Concise description

Cover:
- Nominal cases
- Boundary conditions
- Error scenarios
- State transitions

Return ONLY JSON: {"test_cases": [{"id": "TC-001", "description": "..."}]}"""
                ),
                UserMessage(
                    content=f"""Requirement: {state.req_name}

Description:
{state.requirement_description}"""
                ),
            ],
            temperature=runtime.context.temperature,
            response_format=TestCaseList,
        )

        if not response.choices or not response.choices[0].message:
            raise ValueError("No response from Mistral")

        parsed = response.choices[0].message.parsed
        if not parsed:
            raise ValueError("No parsed response")

        return {
            "generated_test_cases": [
                f"{tc.id}: {tc.description}" for tc in parsed.test_cases
            ]
        }
    except Exception as e:
        return {"errors": [*state.errors, f"LLM1 error: {e}"]}


async def load_current_scenario(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:  # noqa: ARG001
    """Load the content of the current XML scenario."""
    if state.errors or state.current_scenario_index >= len(state.scenario_paths):
        return {}

    path = state.scenario_paths[state.current_scenario_index]
    try:
        content = await asyncio.to_thread(Path(path).read_text, encoding="utf-8")
        return {
            "current_scenario_content": content,
            "current_scenario_name": Path(path).stem,
        }
    except Exception as e:
        return {"errors": [*state.errors, f"Error reading {path}: {e}"]}


async def analyze_test_scenario(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:
    """Analyze the scenario and mark covered test cases."""
    if (
        state.errors
        or not state.generated_test_cases
        or not state.current_scenario_content
    ):
        return {}

    try:
        client = get_mistral_client()
        response = await client.chat.parse_async(
            model=runtime.context.llm2_model,
            messages=[
                SystemMessage(
                    content="""You are a test analysis expert.
Analyze an XML scenario and determine which test cases are covered.

Set present=true if the test case IS verified by the scenario, false otherwise.

Return ONLY JSON: {"test_cases": [{"id": "TC-001", "description": "...", "present": true}]}"""
                ),
                UserMessage(
                    content=f"""Requirement: {state.req_name}
Scenario: {state.current_scenario_name}

Test Cases:
{chr(10).join(f"- {tc}" for tc in state.generated_test_cases)}

Scenario XML:
```xml
{state.current_scenario_content}
```"""
                ),
            ],
            temperature=runtime.context.temperature,
            response_format=TestCaseAnalysis,
        )

        if not response.choices or not response.choices[0].message:
            raise ValueError("No response from Mistral")

        parsed = response.choices[0].message.parsed
        if not parsed:
            raise ValueError("No parsed response")

        result: ScenarioResult = {
            "scenario_name": state.current_scenario_name,
            "scenario_path": state.scenario_paths[state.current_scenario_index],
            "test_cases": [
                {"id": tc.id, "description": tc.description, "present": tc.present}
                for tc in parsed.test_cases
            ],
        }
        return {"scenario_results": [*state.scenario_results, result]}
    except Exception as e:
        return {"errors": [*state.errors, f"LLM2 error: {e}"]}


async def move_to_next_scenario(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:  # noqa: ARG001
    """Move to the next scenario."""
    return {"current_scenario_index": state.current_scenario_index + 1}


async def aggregate_test_cases(
    state: State, runtime: Runtime[Context]
) -> dict[str, Any]:  # noqa: ARG001
    """Aggregate test cases with OR logic on present status."""
    if not state.scenario_results:
        return {"aggregated_test_cases": []}

    aggregated: dict[str, TestCase] = {}
    for result in state.scenario_results:
        for tc in result["test_cases"]:
            if tc["id"] not in aggregated:
                aggregated[tc["id"]] = {
                    "id": tc["id"],
                    "description": tc["description"],
                    "present": tc["present"],
                }
            else:
                aggregated[tc["id"]]["present"] = (
                    aggregated[tc["id"]]["present"] or tc["present"]
                )

    return {"aggregated_test_cases": list(aggregated.values())}


def has_more_scenarios(state: State) -> str:
    """Determine whether to continue or end."""
    if state.errors:
        return "end"
    return (
        "continue"
        if state.current_scenario_index < len(state.scenario_paths)
        else "end"
    )


# =============================================================================
# Graph
# =============================================================================

graph = (
    StateGraph(State, context_schema=Context)
    .add_node("load_scenarios", load_scenarios)
    .add_node("retrieve_requirement", retrieve_requirement)
    .add_node("generate_test_cases", generate_test_cases)
    .add_node("load_current_scenario", load_current_scenario)
    .add_node("analyze_test_scenario", analyze_test_scenario)
    .add_node("move_to_next_scenario", move_to_next_scenario)
    .add_node("aggregate_test_cases", aggregate_test_cases)
    .add_edge(START, "load_scenarios")
    .add_edge("load_scenarios", "retrieve_requirement")
    .add_edge("retrieve_requirement", "generate_test_cases")
    .add_edge("generate_test_cases", "load_current_scenario")
    .add_edge("load_current_scenario", "analyze_test_scenario")
    .add_edge("analyze_test_scenario", "move_to_next_scenario")
    .add_conditional_edges(
        "move_to_next_scenario",
        has_more_scenarios,
        {"continue": "load_current_scenario", "end": "aggregate_test_cases"},
    )
    .add_edge("aggregate_test_cases", END)
    .compile(name="Test Coverage Pipeline")
)
